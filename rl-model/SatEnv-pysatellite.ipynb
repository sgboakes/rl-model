{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "516a7319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jan  20 13:29:40 2022\n",
    "\n",
    "@author: sgboakes\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pysatellite import Transformations, Functions, Filters\n",
    "import pysatellite.config as cfg\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    plt.close('all')\n",
    "    # ~~~~ Variables\n",
    "\n",
    "    sin = np.sin\n",
    "    cos = np.cos\n",
    "    pi = np.float64(np.pi)\n",
    "\n",
    "    sensLat = np.float64(28.300697)\n",
    "    sensLon = np.float64(-16.509675)\n",
    "    sensAlt = np.float64(2390)\n",
    "    sensLLA = np.array([[sensLat * pi / 180], [sensLon * pi / 180], [sensAlt]], dtype='float64')\n",
    "    # sensLLA = np.array([[pi/2], [0], [1000]], dtype='float64')\n",
    "    sensECEF = Transformations.LLAtoECEF(sensLLA)\n",
    "    sensECEF.shape = (3, 1)\n",
    "\n",
    "    simLength = cfg.simLength\n",
    "    stepLength = cfg.stepLength\n",
    "\n",
    "    mu = cfg.mu\n",
    "\n",
    "    trans_earth = True\n",
    "\n",
    "    # ~~~~ Satellite Conversion\n",
    "\n",
    "    # Define sat pos in ECI and convert to AER\n",
    "    # radArr: radii for each sat metres\n",
    "    # omegaArr: orbital rate for each sat rad/s\n",
    "    # thetaArr: inclination angle for each sat rad\n",
    "    # kArr: normal vector for each sat metres\n",
    "\n",
    "    radArr = np.array([7e6, 7e6, 7e6, 7e6], dtype='float64')\n",
    "\n",
    "    omegaArr = 1 / np.sqrt(radArr ** 3 / mu)\n",
    "\n",
    "    thetaArr = np.array([[0.0], [0.1], [0.2], [0.3]], dtype='float64')\n",
    "\n",
    "    kArr = np.array([[0, 0, 0],\n",
    "                     [0, 0, 1],\n",
    "                     [1 / np.sqrt(2), 1 / np.sqrt(2), 0],\n",
    "                     [1 / np.sqrt(3), 1 / np.sqrt(3), 1 / np.sqrt(3)]],\n",
    "                    dtype='float64')\n",
    "\n",
    "    num_sats = len(radArr)\n",
    "\n",
    "    # Make data structures\n",
    "    satECI = {chr(i + 97): np.zeros((3, simLength)) for i in range(num_sats)}\n",
    "    satAER = {chr(i + 97): np.zeros((3, simLength)) for i in range(num_sats)}\n",
    "\n",
    "    for i in range(num_sats):\n",
    "        c = chr(i + 97)\n",
    "        for j in range(simLength):\n",
    "            v = np.array([[radArr[i] * sin(omegaArr[i] * (j + 1) * stepLength)],\n",
    "                          [0],\n",
    "                          [radArr[i] * cos(omegaArr[i] * (j + 1) * stepLength)]], dtype='float64')\n",
    "\n",
    "            satECI[c][:, j] = (v @ cos(thetaArr[i])) + (np.cross(kArr[i, :].T, v.T) * sin(thetaArr[i])) + (\n",
    "                        kArr[i, :].T * np.dot(kArr[i, :].T, v) * (1 - cos(thetaArr[i])))\n",
    "\n",
    "            satAER[c][:, j:j + 1] = Transformations.ECItoAER(satECI[c][:, j], stepLength, j + 1, sensECEF, sensLLA[0],\n",
    "                                                             sensLLA[1])\n",
    "\n",
    "            if not trans_earth:\n",
    "                if satAER[c][1, j] < 0:\n",
    "                    satAER[c][:, j:j + 1] = np.array([[np.nan], [np.nan], [np.nan]])\n",
    "\n",
    "        if np.isnan(satAER[c]).all():\n",
    "            print('Satellite {s} is not observable'.format(s=c))\n",
    "\n",
    "    # Add small deviations for measurements\n",
    "    # Using calculated max measurement deviations for LT:\n",
    "    # Based on 0.15\"/pixel, sat size = 2m, max range = 1.38e7\n",
    "    # sigma = 1/2 * 0.15\" for it to be definitely on that pixel\n",
    "    # Add angle devs to Az/Elev, and range devs to Range\n",
    "\n",
    "    angMeasDev, rangeMeasDev = 1e-6, 20\n",
    "\n",
    "    satAERMes = {chr(i + 97): np.zeros((3, simLength)) for i in range(num_sats)}\n",
    "    for i in range(num_sats):\n",
    "        c = chr(i + 97)\n",
    "        satAERMes[c][0, :] = satAER[c][0, :] + (angMeasDev * np.random.randn(1, simLength))\n",
    "        satAERMes[c][1, :] = satAER[c][1, :] + (angMeasDev * np.random.randn(1, simLength))\n",
    "        satAERMes[c][2, :] = satAER[c][2, :] + (rangeMeasDev * np.random.randn(1, simLength))\n",
    "\n",
    "    satECIMes = {chr(i + 97): np.zeros((3, simLength)) for i in range(num_sats)}\n",
    "    for i in range(num_sats):\n",
    "        c = chr(i + 97)\n",
    "        for j in range(simLength):\n",
    "            satECIMes[c][:, j:j + 1] = Transformations.AERtoECI(satAERMes[c][:, j], stepLength, j+1, sensECEF,\n",
    "                                                                sensLLA[0], sensLLA[1])\n",
    "\n",
    "    satState = {chr(i + 97): np.zeros((6, 1)) for i in range(num_sats)}\n",
    "    for i in range(num_sats):\n",
    "        c = chr(i + 97)\n",
    "        for j in range(simLength):\n",
    "            if np.all(np.isnan(satECIMes[c][:, j])):\n",
    "                continue\n",
    "            else:\n",
    "                satState[c][0:3] = np.reshape(satECIMes[c][:, j], (3, 1))\n",
    "                break\n",
    "\n",
    "    # Process noise\n",
    "    stdAng = np.float64(1e-5)\n",
    "    coefA = np.float64(0.25 * stepLength ** 4.0 * stdAng ** 2.0)\n",
    "    coefB = np.float64(stepLength ** 2.0 * stdAng ** 2.0)\n",
    "    coefC = np.float64(0.5 * stepLength ** 3.0 * stdAng ** 2.0)\n",
    "\n",
    "    procNoise = np.array([[coefA, 0, 0, coefC, 0, 0],\n",
    "                          [0, coefA, 0, 0, coefC, 0],\n",
    "                          [0, 0, coefA, 0, 0, coefC],\n",
    "                          [coefC, 0, 0, coefB, 0, 0],\n",
    "                          [0, coefC, 0, 0, coefB, 0],\n",
    "                          [0, 0, coefC, 0, 0, coefB]],\n",
    "                         dtype='float64')\n",
    "\n",
    "    covState = {chr(i + 97): np.zeros((6, 6)) for i in range(num_sats)}\n",
    "    for i in range(num_sats):\n",
    "        c = chr(i + 97)\n",
    "        covState[c] = np.float64(1e10) * np.identity(6)\n",
    "\n",
    "    covAER = np.array([[(angMeasDev * 180 / pi) ** 2, 0, 0],\n",
    "                       [0, (angMeasDev * 180 / pi) ** 2, 0],\n",
    "                       [0, 0, rangeMeasDev ** 2]],\n",
    "                      dtype='float64'\n",
    "                      )\n",
    "\n",
    "    measureMatrix = np.append(np.identity(3), np.zeros((3, 3)), axis=1)\n",
    "\n",
    "    totalStates = {chr(i + 97): np.zeros((6, simLength)) for i in range(num_sats)}\n",
    "    diffState = {chr(i + 97): np.zeros((3, simLength)) for i in range(num_sats)}\n",
    "    err_X_ECI = {chr(i + 97): np.zeros(simLength) for i in range(num_sats)}\n",
    "    err_Y_ECI = {chr(i + 97): np.zeros(simLength) for i in range(num_sats)}\n",
    "    err_Z_ECI = {chr(i + 97): np.zeros(simLength) for i in range(num_sats)}\n",
    "\n",
    "    # ~~~~~ Using EKF\n",
    "\n",
    "    delta = 1e-6\n",
    "    for i in range(num_sats):\n",
    "        c = chr(i + 97)\n",
    "        mesCheck = False\n",
    "        for j in range(simLength):\n",
    "            while not mesCheck:\n",
    "                if np.all(np.isnan(satECIMes[c][:, j])):\n",
    "                    break\n",
    "                else:\n",
    "                    mesCheck = True\n",
    "                    break\n",
    "\n",
    "            if not mesCheck:\n",
    "                continue\n",
    "\n",
    "            func_params = {\n",
    "                \"stepLength\": stepLength,\n",
    "                \"count\": j + 1,\n",
    "                \"sensECEF\": sensECEF,\n",
    "                \"sensLLA[0]\": sensLLA[0],\n",
    "                \"sensLLA[1]\": sensLLA[1]\n",
    "            }\n",
    "\n",
    "            jacobian = Functions.jacobian_finder(\"AERtoECI\", np.reshape(satAERMes[c][:, j], (3, 1)), func_params, delta)\n",
    "\n",
    "            # covECI = np.matmul(np.matmul(jacobian, covAER), jacobian.T)\n",
    "            covECI = jacobian @ covAER @ jacobian.T\n",
    "\n",
    "            stateTransMatrix = Functions.jacobian_finder(\"kepler\", satState[c], [], delta)\n",
    "\n",
    "            satState[c], covState[c] = Filters.EKF_ECI(satState[c], covState[c], satECIMes[c][:, j], stateTransMatrix,\n",
    "                                                       measureMatrix, covECI, procNoise)\n",
    "\n",
    "            totalStates[c][:, j] = np.reshape(satState[c], 6)\n",
    "            err_X_ECI[c][j] = (np.sqrt(np.abs(covState[c][0, 0])))\n",
    "            err_Y_ECI[c][j] = (np.sqrt(np.abs(covState[c][1, 1])))\n",
    "            err_Z_ECI[c][j] = (np.sqrt(np.abs(covState[c][2, 2])))\n",
    "            diffState[c][:, j] = totalStates[c][0:3, j] - satECI[c][:, j]\n",
    "            # print(satState[c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307f0244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 13:35:34.498463: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-20 13:35:34.498480: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "# import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import PIL.Image\n",
    "# import pyvirtualdisplay\n",
    "import reverb\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1685775",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration =   1# @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f608313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simlength 500 current ep 0\n",
      "simlength 500 current ep 1\n",
      "simlength 500 current ep 2\n",
      "simlength 500 current ep 3\n",
      "simlength 500 current ep 4\n",
      "simlength 500 current ep 5\n",
      "simlength 500 current ep 6\n",
      "simlength 500 current ep 7\n",
      "simlength 500 current ep 8\n",
      "simlength 500 current ep 9\n",
      "simlength 500 current ep 10\n",
      "simlength 500 current ep 11\n",
      "simlength 500 current ep 12\n",
      "simlength 500 current ep 13\n",
      "simlength 500 current ep 14\n",
      "simlength 500 current ep 15\n",
      "simlength 500 current ep 16\n",
      "simlength 500 current ep 17\n",
      "simlength 500 current ep 18\n",
      "simlength 500 current ep 19\n",
      "simlength 500 current ep 20\n",
      "simlength 500 current ep 21\n",
      "simlength 500 current ep 22\n",
      "simlength 500 current ep 23\n",
      "simlength 500 current ep 24\n",
      "simlength 500 current ep 25\n",
      "simlength 500 current ep 26\n",
      "simlength 500 current ep 27\n",
      "simlength 500 current ep 28\n",
      "simlength 500 current ep 29\n",
      "simlength 500 current ep 30\n",
      "simlength 500 current ep 31\n",
      "simlength 500 current ep 32\n",
      "simlength 500 current ep 33\n",
      "simlength 500 current ep 34\n",
      "simlength 500 current ep 35\n",
      "simlength 500 current ep 36\n",
      "simlength 500 current ep 37\n",
      "simlength 500 current ep 38\n",
      "simlength 500 current ep 39\n",
      "simlength 500 current ep 40\n",
      "simlength 500 current ep 41\n",
      "simlength 500 current ep 42\n",
      "simlength 500 current ep 43\n",
      "simlength 500 current ep 44\n",
      "simlength 500 current ep 45\n",
      "simlength 500 current ep 46\n",
      "simlength 500 current ep 47\n",
      "simlength 500 current ep 48\n",
      "simlength 500 current ep 49\n",
      "simlength 500 current ep 50\n",
      "simlength 500 current ep 51\n",
      "simlength 500 current ep 52\n",
      "simlength 500 current ep 53\n",
      "simlength 500 current ep 54\n",
      "simlength 500 current ep 55\n",
      "simlength 500 current ep 56\n",
      "simlength 500 current ep 57\n",
      "simlength 500 current ep 58\n",
      "simlength 500 current ep 59\n",
      "simlength 500 current ep 60\n",
      "simlength 500 current ep 61\n",
      "simlength 500 current ep 62\n",
      "simlength 500 current ep 63\n",
      "simlength 500 current ep 64\n",
      "simlength 500 current ep 65\n",
      "simlength 500 current ep 66\n",
      "simlength 500 current ep 67\n",
      "simlength 500 current ep 68\n",
      "simlength 500 current ep 69\n",
      "simlength 500 current ep 70\n",
      "simlength 500 current ep 71\n",
      "simlength 500 current ep 72\n",
      "simlength 500 current ep 73\n",
      "simlength 500 current ep 74\n",
      "simlength 500 current ep 75\n",
      "simlength 500 current ep 76\n",
      "simlength 500 current ep 77\n",
      "simlength 500 current ep 78\n",
      "simlength 500 current ep 79\n",
      "simlength 500 current ep 80\n",
      "simlength 500 current ep 81\n",
      "simlength 500 current ep 82\n",
      "simlength 500 current ep 83\n",
      "simlength 500 current ep 84\n",
      "simlength 500 current ep 85\n",
      "simlength 500 current ep 86\n",
      "simlength 500 current ep 87\n",
      "simlength 500 current ep 88\n",
      "simlength 500 current ep 89\n",
      "simlength 500 current ep 90\n",
      "simlength 500 current ep 91\n",
      "simlength 500 current ep 92\n",
      "simlength 500 current ep 93\n",
      "simlength 500 current ep 94\n",
      "simlength 500 current ep 95\n",
      "simlength 500 current ep 96\n",
      "simlength 500 current ep 97\n",
      "simlength 500 current ep 98\n",
      "simlength 500 current ep 99\n",
      "simlength 500 current ep 100\n",
      "simlength 500 current ep 101\n",
      "simlength 500 current ep 102\n",
      "simlength 500 current ep 103\n",
      "simlength 500 current ep 104\n",
      "simlength 500 current ep 105\n",
      "simlength 500 current ep 106\n",
      "simlength 500 current ep 107\n",
      "simlength 500 current ep 108\n",
      "simlength 500 current ep 109\n",
      "simlength 500 current ep 110\n",
      "simlength 500 current ep 111\n",
      "simlength 500 current ep 112\n",
      "simlength 500 current ep 113\n",
      "simlength 500 current ep 114\n",
      "simlength 500 current ep 115\n",
      "simlength 500 current ep 116\n",
      "simlength 500 current ep 117\n",
      "simlength 500 current ep 118\n",
      "simlength 500 current ep 119\n",
      "simlength 500 current ep 120\n",
      "simlength 500 current ep 121\n",
      "simlength 500 current ep 122\n",
      "simlength 500 current ep 123\n",
      "simlength 500 current ep 124\n",
      "simlength 500 current ep 125\n",
      "simlength 500 current ep 126\n",
      "simlength 500 current ep 127\n",
      "simlength 500 current ep 128\n",
      "simlength 500 current ep 129\n",
      "simlength 500 current ep 130\n",
      "simlength 500 current ep 131\n",
      "simlength 500 current ep 132\n",
      "simlength 500 current ep 133\n",
      "simlength 500 current ep 134\n",
      "simlength 500 current ep 135\n",
      "simlength 500 current ep 136\n",
      "simlength 500 current ep 137\n",
      "simlength 500 current ep 138\n",
      "simlength 500 current ep 139\n",
      "simlength 500 current ep 140\n",
      "simlength 500 current ep 141\n",
      "simlength 500 current ep 142\n",
      "simlength 500 current ep 143\n",
      "simlength 500 current ep 144\n",
      "simlength 500 current ep 145\n",
      "simlength 500 current ep 146\n",
      "simlength 500 current ep 147\n",
      "simlength 500 current ep 148\n",
      "simlength 500 current ep 149\n",
      "simlength 500 current ep 150\n",
      "simlength 500 current ep 151\n",
      "simlength 500 current ep 152\n",
      "simlength 500 current ep 153\n",
      "simlength 500 current ep 154\n",
      "simlength 500 current ep 155\n",
      "simlength 500 current ep 156\n",
      "simlength 500 current ep 157\n",
      "simlength 500 current ep 158\n",
      "simlength 500 current ep 159\n",
      "simlength 500 current ep 160\n",
      "simlength 500 current ep 161\n",
      "simlength 500 current ep 162\n",
      "simlength 500 current ep 163\n",
      "simlength 500 current ep 164\n",
      "simlength 500 current ep 165\n",
      "simlength 500 current ep 166\n",
      "simlength 500 current ep 167\n",
      "simlength 500 current ep 168\n",
      "simlength 500 current ep 169\n",
      "simlength 500 current ep 170\n",
      "simlength 500 current ep 171\n",
      "simlength 500 current ep 172\n",
      "simlength 500 current ep 173\n",
      "simlength 500 current ep 174\n",
      "simlength 500 current ep 175\n",
      "simlength 500 current ep 176\n",
      "simlength 500 current ep 177\n",
      "simlength 500 current ep 178\n",
      "simlength 500 current ep 179\n",
      "simlength 500 current ep 180\n",
      "simlength 500 current ep 181\n",
      "simlength 500 current ep 182\n",
      "simlength 500 current ep 183\n",
      "simlength 500 current ep 184\n",
      "simlength 500 current ep 185\n",
      "simlength 500 current ep 186\n",
      "simlength 500 current ep 187\n",
      "simlength 500 current ep 188\n",
      "simlength 500 current ep 189\n",
      "simlength 500 current ep 190\n",
      "simlength 500 current ep 191\n",
      "simlength 500 current ep 192\n",
      "simlength 500 current ep 193\n",
      "simlength 500 current ep 194\n",
      "simlength 500 current ep 195\n",
      "simlength 500 current ep 196\n",
      "simlength 500 current ep 197\n",
      "simlength 500 current ep 198\n",
      "simlength 500 current ep 199\n",
      "simlength 500 current ep 200\n",
      "simlength 500 current ep 201\n",
      "simlength 500 current ep 202\n",
      "simlength 500 current ep 203\n",
      "simlength 500 current ep 204\n",
      "simlength 500 current ep 205\n",
      "simlength 500 current ep 206\n",
      "simlength 500 current ep 207\n",
      "simlength 500 current ep 208\n",
      "simlength 500 current ep 209\n",
      "simlength 500 current ep 210\n",
      "simlength 500 current ep 211\n",
      "simlength 500 current ep 212\n",
      "simlength 500 current ep 213\n",
      "simlength 500 current ep 214\n",
      "simlength 500 current ep 215\n",
      "simlength 500 current ep 216\n",
      "simlength 500 current ep 217\n",
      "simlength 500 current ep 218\n",
      "simlength 500 current ep 219\n",
      "simlength 500 current ep 220\n",
      "simlength 500 current ep 221\n",
      "simlength 500 current ep 222\n",
      "simlength 500 current ep 223\n",
      "simlength 500 current ep 224\n",
      "simlength 500 current ep 225\n",
      "simlength 500 current ep 226\n",
      "simlength 500 current ep 227\n",
      "simlength 500 current ep 228\n",
      "simlength 500 current ep 229\n",
      "simlength 500 current ep 230\n",
      "simlength 500 current ep 231\n",
      "simlength 500 current ep 232\n",
      "simlength 500 current ep 233\n",
      "simlength 500 current ep 234\n",
      "simlength 500 current ep 235\n",
      "simlength 500 current ep 236\n",
      "simlength 500 current ep 237\n",
      "simlength 500 current ep 238\n",
      "simlength 500 current ep 239\n",
      "simlength 500 current ep 240\n",
      "simlength 500 current ep 241\n",
      "simlength 500 current ep 242\n",
      "simlength 500 current ep 243\n",
      "simlength 500 current ep 244\n",
      "simlength 500 current ep 245\n",
      "simlength 500 current ep 246\n",
      "simlength 500 current ep 247\n",
      "simlength 500 current ep 248\n",
      "simlength 500 current ep 249\n",
      "simlength 500 current ep 250\n",
      "simlength 500 current ep 251\n",
      "simlength 500 current ep 252\n",
      "simlength 500 current ep 253\n",
      "simlength 500 current ep 254\n",
      "simlength 500 current ep 255\n",
      "simlength 500 current ep 256\n",
      "simlength 500 current ep 257\n",
      "simlength 500 current ep 258\n",
      "simlength 500 current ep 259\n",
      "simlength 500 current ep 260\n",
      "simlength 500 current ep 261\n",
      "simlength 500 current ep 262\n",
      "simlength 500 current ep 263\n",
      "simlength 500 current ep 264\n",
      "simlength 500 current ep 265\n",
      "simlength 500 current ep 266\n",
      "simlength 500 current ep 267\n",
      "simlength 500 current ep 268\n",
      "simlength 500 current ep 269\n",
      "simlength 500 current ep 270\n",
      "simlength 500 current ep 271\n",
      "simlength 500 current ep 272\n",
      "simlength 500 current ep 273\n",
      "simlength 500 current ep 274\n",
      "simlength 500 current ep 275\n",
      "simlength 500 current ep 276\n",
      "simlength 500 current ep 277\n",
      "simlength 500 current ep 278\n",
      "simlength 500 current ep 279\n",
      "simlength 500 current ep 280\n",
      "simlength 500 current ep 281\n",
      "simlength 500 current ep 282\n",
      "simlength 500 current ep 283\n",
      "simlength 500 current ep 284\n",
      "simlength 500 current ep 285\n",
      "simlength 500 current ep 286\n",
      "simlength 500 current ep 287\n",
      "simlength 500 current ep 288\n",
      "simlength 500 current ep 289\n",
      "simlength 500 current ep 290\n",
      "simlength 500 current ep 291\n",
      "simlength 500 current ep 292\n",
      "simlength 500 current ep 293\n",
      "simlength 500 current ep 294\n",
      "simlength 500 current ep 295\n",
      "simlength 500 current ep 296\n",
      "simlength 500 current ep 297\n",
      "simlength 500 current ep 298\n",
      "simlength 500 current ep 299\n",
      "simlength 500 current ep 300\n",
      "simlength 500 current ep 301\n",
      "simlength 500 current ep 302\n",
      "simlength 500 current ep 303\n",
      "simlength 500 current ep 304\n",
      "simlength 500 current ep 305\n",
      "simlength 500 current ep 306\n",
      "simlength 500 current ep 307\n",
      "simlength 500 current ep 308\n",
      "simlength 500 current ep 309\n",
      "simlength 500 current ep 310\n",
      "simlength 500 current ep 311\n",
      "simlength 500 current ep 312\n",
      "simlength 500 current ep 313\n",
      "simlength 500 current ep 314\n",
      "simlength 500 current ep 315\n",
      "simlength 500 current ep 316\n",
      "simlength 500 current ep 317\n",
      "simlength 500 current ep 318\n",
      "simlength 500 current ep 319\n",
      "simlength 500 current ep 320\n",
      "simlength 500 current ep 321\n",
      "simlength 500 current ep 322\n",
      "simlength 500 current ep 323\n",
      "simlength 500 current ep 324\n",
      "simlength 500 current ep 325\n",
      "simlength 500 current ep 326\n",
      "simlength 500 current ep 327\n",
      "simlength 500 current ep 328\n",
      "simlength 500 current ep 329\n",
      "simlength 500 current ep 330\n",
      "simlength 500 current ep 331\n",
      "simlength 500 current ep 332\n",
      "simlength 500 current ep 333\n",
      "simlength 500 current ep 334\n",
      "simlength 500 current ep 335\n",
      "simlength 500 current ep 336\n",
      "simlength 500 current ep 337\n",
      "simlength 500 current ep 338\n",
      "simlength 500 current ep 339\n",
      "simlength 500 current ep 340\n",
      "simlength 500 current ep 341\n",
      "simlength 500 current ep 342\n",
      "simlength 500 current ep 343\n",
      "simlength 500 current ep 344\n",
      "simlength 500 current ep 345\n",
      "simlength 500 current ep 346\n",
      "simlength 500 current ep 347\n",
      "simlength 500 current ep 348\n",
      "simlength 500 current ep 349\n",
      "simlength 500 current ep 350\n",
      "simlength 500 current ep 351\n",
      "simlength 500 current ep 352\n",
      "simlength 500 current ep 353\n",
      "simlength 500 current ep 354\n",
      "simlength 500 current ep 355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simlength 500 current ep 356\n",
      "simlength 500 current ep 357\n",
      "simlength 500 current ep 358\n",
      "simlength 500 current ep 359\n",
      "simlength 500 current ep 360\n",
      "simlength 500 current ep 361\n",
      "simlength 500 current ep 362\n",
      "simlength 500 current ep 363\n",
      "simlength 500 current ep 364\n",
      "simlength 500 current ep 365\n",
      "simlength 500 current ep 366\n",
      "simlength 500 current ep 367\n",
      "simlength 500 current ep 368\n",
      "simlength 500 current ep 369\n",
      "simlength 500 current ep 370\n",
      "simlength 500 current ep 371\n",
      "simlength 500 current ep 372\n",
      "simlength 500 current ep 373\n",
      "simlength 500 current ep 374\n",
      "simlength 500 current ep 375\n",
      "simlength 500 current ep 376\n",
      "simlength 500 current ep 377\n",
      "simlength 500 current ep 378\n",
      "simlength 500 current ep 379\n",
      "simlength 500 current ep 380\n",
      "simlength 500 current ep 381\n",
      "simlength 500 current ep 382\n",
      "simlength 500 current ep 383\n",
      "simlength 500 current ep 384\n",
      "simlength 500 current ep 385\n",
      "simlength 500 current ep 386\n",
      "simlength 500 current ep 387\n",
      "simlength 500 current ep 388\n",
      "simlength 500 current ep 389\n",
      "simlength 500 current ep 390\n",
      "simlength 500 current ep 391\n",
      "simlength 500 current ep 392\n",
      "simlength 500 current ep 393\n",
      "simlength 500 current ep 394\n",
      "simlength 500 current ep 395\n",
      "simlength 500 current ep 396\n",
      "simlength 500 current ep 397\n",
      "simlength 500 current ep 398\n",
      "simlength 500 current ep 399\n",
      "simlength 500 current ep 400\n",
      "simlength 500 current ep 401\n",
      "simlength 500 current ep 402\n",
      "simlength 500 current ep 403\n",
      "simlength 500 current ep 404\n",
      "simlength 500 current ep 405\n",
      "simlength 500 current ep 406\n",
      "simlength 500 current ep 407\n",
      "simlength 500 current ep 408\n",
      "simlength 500 current ep 409\n",
      "simlength 500 current ep 410\n",
      "simlength 500 current ep 411\n",
      "simlength 500 current ep 412\n",
      "simlength 500 current ep 413\n",
      "simlength 500 current ep 414\n",
      "simlength 500 current ep 415\n",
      "simlength 500 current ep 416\n",
      "simlength 500 current ep 417\n",
      "simlength 500 current ep 418\n",
      "simlength 500 current ep 419\n",
      "simlength 500 current ep 420\n",
      "simlength 500 current ep 421\n",
      "simlength 500 current ep 422\n",
      "simlength 500 current ep 423\n",
      "simlength 500 current ep 424\n",
      "simlength 500 current ep 425\n",
      "simlength 500 current ep 426\n",
      "simlength 500 current ep 427\n",
      "simlength 500 current ep 428\n",
      "simlength 500 current ep 429\n",
      "simlength 500 current ep 430\n",
      "simlength 500 current ep 431\n",
      "simlength 500 current ep 432\n",
      "simlength 500 current ep 433\n",
      "simlength 500 current ep 434\n",
      "simlength 500 current ep 435\n",
      "simlength 500 current ep 436\n",
      "simlength 500 current ep 437\n",
      "simlength 500 current ep 438\n",
      "simlength 500 current ep 439\n",
      "simlength 500 current ep 440\n",
      "simlength 500 current ep 441\n",
      "simlength 500 current ep 442\n",
      "simlength 500 current ep 443\n",
      "simlength 500 current ep 444\n",
      "simlength 500 current ep 445\n",
      "simlength 500 current ep 446\n",
      "simlength 500 current ep 447\n",
      "simlength 500 current ep 448\n",
      "simlength 500 current ep 449\n",
      "simlength 500 current ep 450\n",
      "simlength 500 current ep 451\n",
      "simlength 500 current ep 452\n",
      "simlength 500 current ep 453\n",
      "simlength 500 current ep 454\n",
      "simlength 500 current ep 455\n",
      "simlength 500 current ep 456\n",
      "simlength 500 current ep 457\n",
      "simlength 500 current ep 458\n",
      "simlength 500 current ep 459\n",
      "simlength 500 current ep 460\n",
      "simlength 500 current ep 461\n",
      "simlength 500 current ep 462\n",
      "simlength 500 current ep 463\n",
      "simlength 500 current ep 464\n",
      "simlength 500 current ep 465\n",
      "simlength 500 current ep 466\n",
      "simlength 500 current ep 467\n",
      "simlength 500 current ep 468\n",
      "simlength 500 current ep 469\n",
      "simlength 500 current ep 470\n",
      "simlength 500 current ep 471\n",
      "simlength 500 current ep 472\n",
      "simlength 500 current ep 473\n",
      "simlength 500 current ep 474\n",
      "simlength 500 current ep 475\n",
      "simlength 500 current ep 476\n",
      "simlength 500 current ep 477\n",
      "simlength 500 current ep 478\n",
      "simlength 500 current ep 479\n",
      "simlength 500 current ep 480\n",
      "simlength 500 current ep 481\n",
      "simlength 500 current ep 482\n",
      "simlength 500 current ep 483\n",
      "simlength 500 current ep 484\n",
      "simlength 500 current ep 485\n",
      "simlength 500 current ep 486\n",
      "simlength 500 current ep 487\n",
      "simlength 500 current ep 488\n",
      "simlength 500 current ep 489\n",
      "simlength 500 current ep 490\n",
      "simlength 500 current ep 491\n",
      "simlength 500 current ep 492\n",
      "simlength 500 current ep 493\n",
      "simlength 500 current ep 494\n",
      "simlength 500 current ep 495\n",
      "simlength 500 current ep 496\n",
      "simlength 500 current ep 497\n",
      "simlength 500 current ep 498\n",
      "simlength 500 current ep 499\n",
      "here\n",
      "simlength 500 current ep 500\n",
      "here\n",
      "simlength 500 current ep 501\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 500 is out of bounds for axis 0 with size 500",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m ts\u001b[38;5;241m.\u001b[39mtransition(\n\u001b[1;32m    116\u001b[0m                 np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32), reward\u001b[38;5;241m=\u001b[39mreward, discount\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    119\u001b[0m env \u001b[38;5;241m=\u001b[39m SatEnv()\n\u001b[0;32m--> 120\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_py_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m tf_env \u001b[38;5;241m=\u001b[39m tf_py_environment\u001b[38;5;241m.\u001b[39mTFPyEnvironment(env)\n\u001b[1;32m    123\u001b[0m time_step \u001b[38;5;241m=\u001b[39m tf_env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/anaconda3/envs/rl-sat/lib/python3.9/site-packages/tf_agents/environments/utils.py:84\u001b[0m, in \u001b[0;36mvalidate_py_environment\u001b[0;34m(environment, episodes, observation_and_action_constraint_splitter)\u001b[0m\n\u001b[1;32m     79\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     80\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGiven `time_step`: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m does not match expected \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     81\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`time_step_spec`: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (time_step, batched_time_step_spec))\n\u001b[1;32m     83\u001b[0m action \u001b[38;5;241m=\u001b[39m random_policy\u001b[38;5;241m.\u001b[39maction(time_step)\u001b[38;5;241m.\u001b[39maction\n\u001b[0;32m---> 84\u001b[0m time_step \u001b[38;5;241m=\u001b[39m \u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m episode_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(time_step\u001b[38;5;241m.\u001b[39mis_last())\n",
      "File \u001b[0;32m~/anaconda3/envs/rl-sat/lib/python3.9/site-packages/tf_agents/environments/py_environment.py:233\u001b[0m, in \u001b[0;36mPyEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_reset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step)):\n\u001b[1;32m    231\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step\n",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36mSatEnv._step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_sats):\n\u001b[1;32m     94\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mchr\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m97\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m     dif[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43merr_X_ECI\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_episode_duration\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     96\u001b[0m                      err_Y_ECI[c][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_episode_duration\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     97\u001b[0m                      err_Z_ECI[c][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_episode_duration\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Make sure episodes don't go on forever.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m dif\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mmax\u001b[39m(dif)):\n",
      "\u001b[0;31mIndexError\u001b[0m: index 500 is out of bounds for axis 0 with size 500"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import int32, float32\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "\n",
    "class PyEnvironment(object):\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Return initial_time_step.\"\"\"\n",
    "        self._current_time_step = self._reset()\n",
    "        return self._current_time_step\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Apply action and return new time_step.\"\"\"\n",
    "        if self._current_time_step is None:\n",
    "            return self.reset()\n",
    "        self._current_time_step = self._step(action)\n",
    "        return self._current_time_step\n",
    "\n",
    "    def current_time_step(self):\n",
    "        return self._current_time_step\n",
    "\n",
    "    def time_step_spec(self):\n",
    "        \"\"\"Return time_step_spec.\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def observation_spec(self):\n",
    "        \"\"\"Return observation_spec.\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def action_spec(self):\n",
    "        \"\"\"Return action_spec.\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _reset(self):\n",
    "        \"\"\"Return initial_time_step.\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _step(self, action):\n",
    "        \"\"\"Apply action and return new time_step.\"\"\"\n",
    "\n",
    "        \n",
    "class SatEnv(py_environment.PyEnvironment):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._num_look_spots = num_sats\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=self._num_look_spots, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(1,), dtype=np.int32, minimum=0, name='observation')\n",
    "        self._state = 0\n",
    "        self._episode_ended = False\n",
    "        self._episode_duration = 0\n",
    "        self._max_episode_length = simLength\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        # self._state = 0\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(np.array([self._state], dtype=np.int32))\n",
    "\n",
    "    def _step(self, action):\n",
    "\n",
    "        reward = 0\n",
    "        if self._episode_ended:\n",
    "            # The last action ended the episode. Ignore the current action and start\n",
    "            # a new episode.\n",
    "            return self.reset()\n",
    "\n",
    "        \n",
    "        \n",
    "        print('simlength {s} current ep {e}'.format(s=simLength,e=self._episode_duration))\n",
    "        dif = [None] * num_sats\n",
    "        for i in range(num_sats):\n",
    "            c = chr(i + 97)\n",
    "            dif[i] = np.sqrt(err_X_ECI[c][self._episode_duration-1]**2 +\n",
    "                             err_Y_ECI[c][self._episode_duration-1]**2 +\n",
    "                             err_Z_ECI[c][self._episode_duration-1]**2)\n",
    "            \n",
    "\n",
    "        # Make sure episodes don't go on forever.\n",
    "        if action == dif.index(max(dif)):\n",
    "            reward = 1\n",
    "\n",
    "        self._state = (self._state + 1) % self._num_look_spots\n",
    "        # print(self._state)\n",
    "        # print(\"reward = \", reward)\n",
    "\n",
    "        self._episode_duration += 1\n",
    "\n",
    "        if self._episode_duration >= self._max_episode_length:\n",
    "            print('here')\n",
    "            self._episode_ended = True\n",
    "            return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
    "        else:\n",
    "            return ts.transition(\n",
    "                np.array([self._state], dtype=np.int32), reward=reward, discount=1.0)\n",
    "\n",
    "\n",
    "env = SatEnv()\n",
    "utils.validate_py_environment(env, episodes=10)\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    while not time_step.is_last():\n",
    "        action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "        time_step = tf_env.step(action)\n",
    "        episode_steps += 1\n",
    "        episode_reward += time_step.reward.numpy()\n",
    "    rewards.append(episode_reward)\n",
    "    steps.append(episode_steps)\n",
    "    time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)\n",
    "\n",
    "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
    "print('avg_length', avg_length, 'avg_reward:', avg_reward)\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b30b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = env\n",
    "eval_py_env = env\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e25429",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (100, 50)\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "  return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# its output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e8c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ea7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409c8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "# See also the metrics module for standard implementations of different metrics.\n",
    "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics\n",
    "\n",
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7738027",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'uniform_table'\n",
    "replay_buffer_signature = tensor_spec.from_spec(\n",
    "      agent.collect_data_spec)\n",
    "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
    "    replay_buffer_signature)\n",
    "\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_buffer_max_length,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "    signature=replay_buffer_signature)\n",
    "\n",
    "reverb_server = reverb.Server([table])\n",
    "\n",
    "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    table_name=table_name,\n",
    "    sequence_length=2,\n",
    "    local_server=reverb_server)\n",
    "\n",
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "  replay_buffer.py_client,\n",
    "  table_name,\n",
    "  sequence_length=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d17956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.collect_data_spec\n",
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa335ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "py_driver.PyDriver(\n",
    "    env,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "      random_policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_steps=initial_collect_steps).run(train_py_env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f1996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "dataset\n",
    "\n",
    "iterator = iter(dataset)\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070fbf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step.\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "# Reset the environment.\n",
    "time_step = train_py_env.reset()\n",
    "\n",
    "# Create a driver to collect experience.\n",
    "collect_driver = py_driver.PyDriver(\n",
    "    env,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "      agent.collect_policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_steps=collect_steps_per_iteration)\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps and save to the replay buffer.\n",
    "  time_step, _ = collect_driver.run(time_step)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "#   if step % log_interval == 0:\n",
    "#     print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9a7f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "\n",
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "# plt.ylim(top=250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
